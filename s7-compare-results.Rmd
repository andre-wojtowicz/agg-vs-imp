---
title: "Results of the experiment"
author: "Andrzej WÃ³jtowicz"
output: 
    html_document:
        keep_md: yes
---

```{r setup, include=FALSE, cache=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = "", 
                      fig.width = 12, fig.height = 8)
source("config.R")
source("utils.R")
library(foreach)
library(knitr)
library(plyr)
library(dplyr)
options(digits = 3)
```

Document generation date: `r Sys.time()`

This document presents results for methods of dealing with missing values
in binary classificaiton problem: uncertaintified classification, imputation and 
aggregation strategies.

# Table of Contents

 1. [Experiment overview](#experiment-overview)
 2. [Datasets](#datasets)
 3. [Original classifiers](#original-classifiers)
 4. [Interval classifiers](#interval-classifiers)
 5. [Imputation methods](#imputation-methods)
 6. [Aggregation strategies](#aggregation-strategies)


***

# Experiment overview

In the problem of classifiaction of ovarian tumor (benign/malignant) we have to deal
with missing values. Instead of doing imputation, mostly because of medical reasons,
one can use former classification models, force them to return interval predictions, 
and summarize these predictions with use of aggregation operators and thresholding strategies.
This approach is described in [Solving the problem of incomplete data in medical diagnosis via interval modeling](https://github.com/ovaexpert/ovarian-tumor-aggregation)
repository. 

In this experiment I check on [UCI Machine Learning datasets](https://github.com/andre-wojtowicz/uci-ml-to-r) how these two approaches
work on real datasets. I use the following datasets: 
`r paste(DATASETS.NAMES, collapse = ", ")`. The experiment procedure looks as follows.

For each dataset $D_i$:

 1. Divide $D_i$ into $D^1$ and $D^2$.
 2. Learn classifiers $K_j$ on $D^1$.
 3. $D_u :=$ randomly obscured $D^2$.
 4. Calculate accuracy, sensitivity, specificity and decisiveness for original 
 classifiers $K_j$ and uncertaintified interval classifiers $\widetilde{K_j}$ on $D_u$.
 5. Choose the best impuation method $\text{Imp}_b$ on $D_u$.
 6. Choose the best aggregation operator $\text{Agg}_b$ on $D_u$:
 
  * for each interval classifier $\widetilde{K_j}$ calculate inteval predictions,
  * choose the best aggregation strategy $\text{Agg}_b$ on interval predictions.
    
 7. Compare original classifiers $K_j$, interval classifiers $\widetilde{K_j}$, 
 the best imputation method $\text{Imp}_b$ and the best aggregation operator $\text{Agg}_b$.
 
In step 2. the classifiers use **different** $D_1$ datasets with `r DATASETS.SIZE.FEATURE.SELECTION` cases for feature selection and `r DATASETS.SIZE.CLASSIFICATION` cases for classification. The further comparison among
classification approaches is done on $D_u$ which is **the same** for all approaches and
consists of `r DATASETS.SIZE.OBSCURATION` cases.

In step 3. `r round(OBSCURATION.NO.NAS.FRACTION * 100, 1)`% of cases are complete.
The remaining are uniformly obscured.
 
All numerical features are preprocessed to be in range $[0, 1]$. In some datasets a few factor attributes are not obscured, in order to reduce
complexity of calculations (see `DATASETS.ALWAYS.AVAILABLE.PREDICTORS` in `config.R`). The random forests are used as feature selection method (classifiaction trees have
internal method). In all learning procedures I use nested `r NCV.FOLDS`-times cross-validation. The performance measure I choose `r tolower(NCV.PERFORMANCE.SELECTOR)`. 
I also use:

 * classification methods: generalized linear models `glm`, neural networks `nnet`, support
 vector machines with linear kernel `svmLinear`, classification trees `rpart`, k-nearest neighbors algorithm `knn` and, as a baseline model, one-rule method `OneR`,
 * imputation methods: `r paste(IMPUTATION.METHODS, collapse = ", ")`,
 * aggregation strategies: the strategies that always return prediction (see `aggregation-operators*.R`).
 
The *decisiveness* performance measure is a proportion of cases for which a classifier
can predict a class.

# Results

```{r results-tables, results="asis"}
for (dataset.name in DATASETS.NAMES)
{
    cat(paste("##", dataset.name, "\n\n"))
    
    cat("### Results on complete dataset\n\n")

    cat("The following tables present predictors used by the original classifiers and 
corresponding performance measures. P-values concern one sample t-tests whether 
mean accuracy across folds is significantly greater than 0.5.\n\n")
    
    dataset.file.path =
            replace.strings(DATASETS.NAME.PATTERN,
                            dataset.name,
                            DATASETS.ORIGIN)
    dataset = readRDS(dataset.file.path)
    
    used.predictors.row.names =
        colnames(dataset)[-ncol(dataset)]
    
    used.predictors = 
        foreach::foreach(model.name = c(CLASSIFIERS.BASELINE, CLASSIFIERS.LIST)) %do%
    {
        model.file.path = 
            replace.strings(c(DATASETS.NAME.PATTERN, CLASSIFIERS.NAME.PATTERN),
                            c(dataset.name, model.name),
                            CLASSIFIERS.LEARNED)
        
        model = suppressWarnings(readRDS(model.file.path))
        
        attr(model, "used.predictors")
    }
    
    names(used.predictors) = c(CLASSIFIERS.BASELINE, CLASSIFIERS.LIST)
    
    up.prnt = 
        used.predictors.as.table(used.predictors, used.predictors.row.names)
    
    cat("Predictors used by classifiers:\n\n")
    
    cat(kable(up.prnt, 
              format = "markdown"), sep = "\n")
    
    cat("\n\n")
    
    df = foreach::foreach(model.name = c(CLASSIFIERS.BASELINE, CLASSIFIERS.LIST),
                          .combine   = rbind) %do%
    {
        model.file.path = 
            replace.strings(c(DATASETS.NAME.PATTERN, CLASSIFIERS.NAME.PATTERN),
                            c(dataset.name, model.name),
                            CLASSIFIERS.LEARNED)
        
        model = suppressWarnings(readRDS(model.file.path))
        
        folds.performance = attr(model, "folds.performance")
        
        pval = t.test(folds.performance$Accuracy, mu = 0.5, alternative = "greater")$p.val
        pval = if (pval < 0.001) {
            " < 0.001 (\\*)"
        } else if (pval < 0.05) {
            paste0(" ", round(pval, 3), " (\\*)")
        } else {
            pval
        }
        
        data.frame(Classifier   = model.name,
                   Accuracy     = mean(folds.performance$Accuracy, na.rm = TRUE),
                   Sensitivity  = mean(folds.performance$Sensitivity, na.rm = TRUE),
                   Specificity  = mean(folds.performance$Specificity, na.rm = TRUE),
                   P.value      = pval)
    }
    
    cat("Performance measures:\n\n")
    
    cat(kable(df, format = "markdown"), sep = "\n")
    
    cat("\n\n")

    cat("### Results on obscured dataset\n\n")
    
    # Original classifiers
    
    df1 = foreach::foreach(model.name = CLASSIFIERS.LIST,
            .combine   = rbind) %do%
    {
        classifier.performance.original.file.path =
            replace.strings(c(DATASETS.NAME.PATTERN, CLASSIFIERS.NAME.PATTERN),
                            c(dataset.name, model.name),
                            CLASSIFIERS.PERFORMANCE.ORIGINAL)
        
        performance.df = readRDS(classifier.performance.original.file.path)
        
        data.frame(Classifier   = model.name,
                   Group        = "Original classifier",
                   Accuracy     = performance.df[1, "Accuracy"],
                   Decisiveness = performance.df[1, "Decisiveness"],
                   Sensitivity  = performance.df[1, "Sensitivity"],
                   Specificity  = performance.df[1, "Specificity"])
    }

    # Uncertaintified classifiers

    df2 = foreach::foreach(model.name = CLASSIFIERS.LIST,
            .combine   = rbind) %do%
    {
        classifier.performance.interval.file.path =
            replace.strings(c(DATASETS.NAME.PATTERN, CLASSIFIERS.NAME.PATTERN),
                            c(dataset.name, model.name),
                            CLASSIFIERS.PERFORMANCE.INTERVAL)
        
        performance.df = readRDS(classifier.performance.interval.file.path)
        
        data.frame(Classifier   = model.name,
                   Group        = "Uncertaintified classifier",
                   Accuracy     = performance.df[1, "Accuracy"],
                   Decisiveness = performance.df[1, "Decisiveness"],
                   Sensitivity  = performance.df[1, "Sensitivity"],
                   Specificity  = performance.df[1, "Specificity"])
    }

    # Imputation

    imputation.model.file.path =
        replace.strings(DATASETS.NAME.PATTERN, dataset.name, CLASSIFIERS.IMPUTATION.MODEL)
    
    imputation.model = suppressWarnings(readRDS(imputation.model.file.path))
    folds.performances = imputation.model$folds.performances
    
    df3 = data.frame(Classifier   = paste(imputation.model$model$method, "&",
                                         imputation.model$imputation.name),
                     Group        = "Imputation",
                     Accuracy     = round(folds.performances %>%
                                          filter(is.na(Missing.attributes)) %>% 
                                              select(Accuracy) %>% unlist %>% mean, 3),
                     Decisiveness = 1.0,
                     Sensitivity  = round(folds.performances %>%
                                          filter(is.na(Missing.attributes)) %>% 
                                              select(Sensitivity) %>% unlist %>% mean, 3),
                     Specificity  = round(folds.performances %>%
                                          filter(is.na(Missing.attributes)) %>% 
                                              select(Specificity) %>% unlist %>% mean, 3))

    # Aggregation strategy 

    agg.model.file.path =
        replace.strings(DATASETS.NAME.PATTERN, dataset.name, AGGREGATION.LEARNED)
    
    agg.model = suppressWarnings(readRDS(agg.model.file.path))
    folds.performances = agg.model$folds.performances
    
    df4 = data.frame(Classifier   = paste0("`", agg.model$aggregation.code,
                                          "` [", agg.model$aggregation.group,
                                          " ",  agg.model$aggregation.subgroup, "]"),
                     Group        = "Aggregation strategy",
                     Accuracy     = round(folds.performances %>%
                                          filter(is.na(Missing.attributes)) %>% 
                                              select(Accuracy) %>% unlist %>% mean, 3),
                     Decisiveness = 1.0,
                     Sensitivity  = round(folds.performances %>%
                                          filter(is.na(Missing.attributes)) %>% 
                                              select(Sensitivity) %>% unlist %>% mean, 3),
                     Specificity  = round(folds.performances %>%
                                          filter(is.na(Missing.attributes)) %>% 
                                              select(Specificity) %>% unlist %>% mean, 3))
    
    cat("Performance measures of original classifiers, interval classifiers, imputation and aggregation strategy:\n\n")
    
    cat(kable(rbind(df1, df2, df3, df4), format = "markdown"), sep = "\n")
    
    cat("\n\n")
}
